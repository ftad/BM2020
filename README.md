# Курс Байесовские методы анализа данных, ФТиАД 2020

### Где и когда
Занятия проходят по понедельникам, 18:10 — 21:00, онлайн (но не всегда).

### Ссылки
Чат в telegram: https://t.me/joinchat/DEBCqh3GZoWaCC8qhYLw1g

[Anytask курса](https://anytask.org/course/751) (инвайт 1hgezx4)

### Правила выставления оценок
В курсе предусмотрено несколько форм контроля знания:
* Домашние работы (на Python/NumPy, ориентировочно две работы) 
* Контрольная работа в середине модуля
* Экзамен

Итоговая оценка вычисляется на основе оценки за работу в семестре и оценки за экзамен:

O<sub>итоговая</sub> = 0.33 * О<sub>домашние задания</sub> + 0.33 * О<sub>контрольная работа</sub>  + 0.34 * О<sub>экзамен</sub>

Оценка за домашнюю работу вычисляется как среднее по домашним заданиям. К итоговой оценке применяется арифметическое округление.

После занятий будут выдаваться теоретические задания для самостоятельной работы. Эти задания не сдаются и не проверяются, но могут встретиться в контрольной работе или экзамене (или похожие задания).

### Экзамен и контрольная работа
[Вопросы к контрольной работе](https://github.com/ftad/BM2020/blob/master/materials/questions_KR.pdf)

Экзамен и контрольная работы письменные (проводятся очно),  работа состоит из теоретических вопросов из списка вопросов и теоретически задач в пропорции 50/50. Продолжительность написания: 1 час 30 минут. На экзамене и контрольной работе можно пользоваться одним листом размера А6 (четверть А4), написанным от руки.

### Правила сдачи домашних заданий

У домашнего задания есть два дедлайна: мягкий и жесткий. Жесткий дедлайн обычно через неделю после мягкого. За сраду после мягкого дедлайна применяется штраф -1 балл за каждый день просрочки. После жесткого дедлайна сдать работу нельзя.

При обнаружении плагиата оценки за домашнее задание обнуляются всем задействованным в списывании студентам. Это очень строгое правило!

### Материалы занятий

__Занятие 1. Введение в байесовские методы. Сопряженные распределения__
* [Презентация](https://github.com/nadiinchi/bm_mini_course_UCM/blob/master/Bayesian_methods_presentation.pdf)
* [Задачи на занятии](https://github.com/nadiinchi/bm_mini_course_UCM/blob/master/Bayesian_methods_problem_set.pdf)
* [__Задание для самостоятельной работы__](https://github.com/ftad/BM2018/blob/master/homeworks/homework2.pdf) (кроме пункта 6)

__Занятие 2. Байесовская линейная регрессия__
* [Презентация](https://github.com/ftad/BM2020/blob/master/materials/presentation_linear_FTAD.pdf)

__Занятие 3. Вариационный вывод и байесовские нейронные сети__
* [Презентация 1 (слайды 82-102)](https://github.com/nadiinchi/bm_mini_course_UCM/blob/master/Bayesian_methods_presentation.pdf)
* [Презентация 2](https://github.com/ftad/BM2020/blob/master/materials/presentation_bnn_ftad.pdf)

__Занятие 4. Методы Markov Chain Monte Carlo__
* [Презентация](https://github.com/ftad/BM2020/blob/master/materials/presentation_MCMC_ftad.pdf)
* [__Задание для самостоятельной работы__](https://github.com/ftad/BM2020/blob/master/materials/homework_BNN.pdf)

__Занятие 5. Гауссовские процессы и байесовская оптимизация__
* [Презентация](https://github.com/ftad/BM2020/blob/master/materials/presentation_GP_ftad.pdf)

__Занятие 6. Модели с латентными переменными и EM-алгоритм__
* [Презентация](https://github.com/ftad/BM2020/blob/master/materials/presentation_EM_ftad.pdf)
* [__Задание для самостоятельной работы__](https://github.com/ftad/BM2020/blob/master/materials/homework_EM.pdf)

### Задания
* [Задание по байесовской линейной регрессии](https://github.com/ftad/BM2020/blob/master/materials/Task_BLR.ipynb). Дедлайн: 23:59 29.09.20.
* [Задание по EM-алгоритму](https://github.com/ftad/BM2020/blob/master/materials/Task_EM.ipynb). Дедлайн: 23:59 24.10.2020.

### Полезные материалы
Книги:
* Barber D. [Bayesian Reasoning and Machine Learning.](http://www0.cs.ucl.ac.uk/staff/d.barber/brml/) Cambridge University Press, 2012.
* Murphy K.P. Machine Learning: A Probabilistic Perspective. The MIT Press, 2012.
* Bishop C.M. [Pattern Recognition and Machine Learning.](http://research.microsoft.com/en-us/um/people/cmbishop/prml/) Springer, 2006. 
* Mackay D.J.C. [Information Theory, Inference, and Learning Algorithms.](http://www.inference.phy.cam.ac.uk/mackay/itila/book.html) Cambridge University Press, 2003. 
* Tipping M. [Sparse Bayesian Learning.](http://www.jmlr.org/papers/volume1/tipping01a/tipping01a.pdf) Journal of Machine Learning Research, 1, 2001, pp. 211-244. 
* Шумский С.А. [Байесова регуляризация обучения.](http://www.niisi.ru/iont/ni/Library/School-2002/Shumsky-2002.pdf) В сб. Лекции по нейроинформатике, часть 2, 2002.

Простые и удобные [заметки](http://cs.nyu.edu/~roweis/notes.html) по матричным вычислениям и свойствам гауссовских распределений.

[Памятка](http://statistics.zone/) по теории вероятностей.
